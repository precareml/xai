{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computational Evaluation of XAI Models using Local Feature Importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step.0 Define Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(filename, model, X_test, y_test, features):    \n",
    "    import os\n",
    "    import re\n",
    "    import pickle\n",
    "    import socket\n",
    "    import datetime\n",
    "        \n",
    "    dir = os.path.dirname(__file__)\n",
    "    now = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "    filepath = os.path.join(dir,\"/Output/RF_Models/\" + filename + \"_\" + now + \".hdfs\")\n",
    "        \n",
    "    model_dict = {\"model\": model, \"X_test\": X_test, \"y_test\": y_test, \"features\": features}\n",
    "        \n",
    "    with open(filepath, 'wb') as f:\n",
    "        pickle.dump(model_dict, f)\n",
    "        \n",
    "    return \"Object has been saved!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_explainer(filename, expl):    \n",
    "    import socket\n",
    "    import pickle\n",
    "    import dill\n",
    "    import os\n",
    "    import re\n",
    "    \n",
    "    dir = os.path.dirname(__file__)\n",
    "    now = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "    filepath = os.path.join(dir,\"/Output/XAI_Models/\" + filename + \"_\"  + now + \".hdfs\")\n",
    "        \n",
    "    xai_model_dict = {\"model\": expl}\n",
    "        \n",
    "    with open(filepath, 'wb') as f:\n",
    "        dill.dump(xai_model_dict, f)\n",
    "        \n",
    "    return \"Object has been saved!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data(filename, size, data):    \n",
    "    import socket\n",
    "    import pickle\n",
    "    import os\n",
    "    import re\n",
    "\n",
    "    dir = os.path.dirname(__file__)\n",
    "    now = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "    filepath = os.path.join(dir,\"/Output/Data/\" + filename+str(size) +  \"_\" + now + \".hdfs\")\n",
    "        \n",
    "    data_dict = {\"data\": data}\n",
    "        \n",
    "    with open(filepath, 'wb') as f:\n",
    "        pickle.dump(data_dict, f)\n",
    "        \n",
    "    return \"Data has been saved!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DeLong CI\n",
    "# resource: https://github.com/yandexdataschool/roc_comparison\n",
    "\n",
    "import numpy as np\n",
    "import scipy.stats\n",
    "from scipy import stats\n",
    "\n",
    "\n",
    "def compute_midrank(x):\n",
    "    \"\"\"Computes midranks.\n",
    "    Args:\n",
    "       x - a 1D numpy array\n",
    "    Returns:\n",
    "       array of midranks\n",
    "    \"\"\"\n",
    "    J = np.argsort(x)\n",
    "    Z = x[J]\n",
    "    N = len(x)\n",
    "    T = np.zeros(N, dtype=np.float64)\n",
    "    i = 0\n",
    "    while i < N:\n",
    "        j = i\n",
    "        while j < N and Z[j] == Z[i]:\n",
    "            j += 1\n",
    "        T[i:j] = 0.5*(i + j - 1)\n",
    "        i = j\n",
    "    T2 = np.empty(N, dtype=np.float64)\n",
    "    # Note(kazeevn) +1 is due to Python using 0-based indexing\n",
    "    # instead of 1-based in the AUC formula in the paper\n",
    "    T2[J] = T + 1\n",
    "    return T2\n",
    "\n",
    "\n",
    "def compute_midrank_weight(x, sample_weight):\n",
    "    \"\"\"Computes midranks.\n",
    "    Args:\n",
    "       x - a 1D numpy array\n",
    "    Returns:\n",
    "       array of midranks\n",
    "    \"\"\"\n",
    "    J = np.argsort(x)\n",
    "    Z = x[J]\n",
    "    cumulative_weight = np.cumsum(sample_weight[J])\n",
    "    N = len(x)\n",
    "    T = np.zeros(N, dtype=np.float64)\n",
    "    i = 0\n",
    "    while i < N:\n",
    "        j = i\n",
    "        while j < N and Z[j] == Z[i]:\n",
    "            j += 1\n",
    "        T[i:j] = cumulative_weight[i:j].mean()\n",
    "        i = j\n",
    "    T2 = np.empty(N, dtype=np.float64)\n",
    "    T2[J] = T\n",
    "    return T2\n",
    "\n",
    "\n",
    "def fastDeLong(predictions_sorted_transposed, label_1_count, sample_weight):\n",
    "    if sample_weight is None:\n",
    "        return fastDeLong_no_weights(predictions_sorted_transposed, label_1_count)\n",
    "    else:\n",
    "        return fastDeLong_weights(predictions_sorted_transposed, label_1_count, sample_weight)\n",
    "\n",
    "\n",
    "def fastDeLong_weights(predictions_sorted_transposed, label_1_count, sample_weight):\n",
    "    \"\"\"\n",
    "    The fast version of DeLong's method for computing the covariance of\n",
    "    unadjusted AUC.\n",
    "    Args:\n",
    "       predictions_sorted_transposed: a 2D numpy.array[n_classifiers, n_examples]\n",
    "          sorted such as the examples with label \"1\" are first\n",
    "    Returns:\n",
    "       (AUC value, DeLong covariance)\n",
    "    Reference:\n",
    "     @article{sun2014fast,\n",
    "       title={Fast Implementation of DeLong's Algorithm for\n",
    "              Comparing the Areas Under Correlated Receiver Oerating Characteristic Curves},\n",
    "       author={Xu Sun and Weichao Xu},\n",
    "       journal={IEEE Signal Processing Letters},\n",
    "       volume={21},\n",
    "       number={11},\n",
    "       pages={1389--1393},\n",
    "       year={2014},\n",
    "       publisher={IEEE}\n",
    "     }\n",
    "    \"\"\"\n",
    "    # Short variables are named as they are in the paper\n",
    "    m = label_1_count\n",
    "    n = predictions_sorted_transposed.shape[1] - m\n",
    "    positive_examples = predictions_sorted_transposed[:, :m]\n",
    "    negative_examples = predictions_sorted_transposed[:, m:]\n",
    "    k = predictions_sorted_transposed.shape[0]\n",
    "\n",
    "    tx = np.empty([k, m], dtype=np.float64)\n",
    "    ty = np.empty([k, n], dtype=np.float64)\n",
    "    tz = np.empty([k, m + n], dtype=np.float64)\n",
    "    for r in range(k):\n",
    "        tx[r, :] = compute_midrank_weight(positive_examples[r, :], sample_weight[:m])\n",
    "        ty[r, :] = compute_midrank_weight(negative_examples[r, :], sample_weight[m:])\n",
    "        tz[r, :] = compute_midrank_weight(predictions_sorted_transposed[r, :], sample_weight)\n",
    "    total_positive_weights = sample_weight[:m].sum()\n",
    "    total_negative_weights = sample_weight[m:].sum()\n",
    "    pair_weights = np.dot(sample_weight[:m, np.newaxis], sample_weight[np.newaxis, m:])\n",
    "    total_pair_weights = pair_weights.sum()\n",
    "    aucs = (sample_weight[:m]*(tz[:, :m] - tx)).sum(axis=1) / total_pair_weights\n",
    "    v01 = (tz[:, :m] - tx[:, :]) / total_negative_weights\n",
    "    v10 = 1. - (tz[:, m:] - ty[:, :]) / total_positive_weights\n",
    "    sx = np.cov(v01)\n",
    "    sy = np.cov(v10)\n",
    "    delongcov = sx / m + sy / n\n",
    "    return aucs, delongcov\n",
    "\n",
    "\n",
    "def fastDeLong_no_weights(predictions_sorted_transposed, label_1_count):\n",
    "    \"\"\"\n",
    "    The fast version of DeLong's method for computing the covariance of\n",
    "    unadjusted AUC.\n",
    "    Args:\n",
    "       predictions_sorted_transposed: a 2D numpy.array[n_classifiers, n_examples]\n",
    "          sorted such as the examples with label \"1\" are first\n",
    "    Returns:\n",
    "       (AUC value, DeLong covariance)\n",
    "    Reference:\n",
    "     @article{sun2014fast,\n",
    "       title={Fast Implementation of DeLong's Algorithm for\n",
    "              Comparing the Areas Under Correlated Receiver Oerating\n",
    "              Characteristic Curves},\n",
    "       author={Xu Sun and Weichao Xu},\n",
    "       journal={IEEE Signal Processing Letters},\n",
    "       volume={21},\n",
    "       number={11},\n",
    "       pages={1389--1393},\n",
    "       year={2014},\n",
    "       publisher={IEEE}\n",
    "     }\n",
    "    \"\"\"\n",
    "    # Short variables are named as they are in the paper\n",
    "    m = label_1_count\n",
    "    n = predictions_sorted_transposed.shape[1] - m\n",
    "    positive_examples = predictions_sorted_transposed[:, :m]\n",
    "    negative_examples = predictions_sorted_transposed[:, m:]\n",
    "    k = predictions_sorted_transposed.shape[0]\n",
    "\n",
    "    tx = np.empty([k, m], dtype=np.float64)\n",
    "    ty = np.empty([k, n], dtype=np.float64)\n",
    "    tz = np.empty([k, m + n], dtype=np.float64)\n",
    "    for r in range(k):\n",
    "        tx[r, :] = compute_midrank(positive_examples[r, :])\n",
    "        ty[r, :] = compute_midrank(negative_examples[r, :])\n",
    "        tz[r, :] = compute_midrank(predictions_sorted_transposed[r, :])\n",
    "    aucs = tz[:, :m].sum(axis=1) / m / n - float(m + 1.0) / 2.0 / n\n",
    "    v01 = (tz[:, :m] - tx[:, :]) / n\n",
    "    v10 = 1.0 - (tz[:, m:] - ty[:, :]) / m\n",
    "    sx = np.cov(v01)\n",
    "    sy = np.cov(v10)\n",
    "    delongcov = sx / m + sy / n\n",
    "    return aucs, delongcov\n",
    "\n",
    "\n",
    "def calc_pvalue(aucs, sigma):\n",
    "    \"\"\"Computes log(10) of p-values.\n",
    "    Args:\n",
    "       aucs: 1D array of AUCs\n",
    "       sigma: AUC DeLong covariances\n",
    "    Returns:\n",
    "       log10(pvalue)\n",
    "    \"\"\"\n",
    "    l = np.array([[1, -1]])\n",
    "    z = np.abs(np.diff(aucs)) / np.sqrt(np.dot(np.dot(l, sigma), l.T))\n",
    "    return np.log10(2) + scipy.stats.norm.logsf(z, loc=0, scale=1) / np.log(10)\n",
    "\n",
    "\n",
    "def compute_ground_truth_statistics(ground_truth, sample_weight):\n",
    "    assert np.array_equal(np.unique(ground_truth), [0, 1])\n",
    "    order = (-ground_truth).argsort()\n",
    "    label_1_count = int(ground_truth.sum())\n",
    "    if sample_weight is None:\n",
    "        ordered_sample_weight = None\n",
    "    else:\n",
    "        ordered_sample_weight = sample_weight[order]\n",
    "\n",
    "    return order, label_1_count, ordered_sample_weight\n",
    "\n",
    "\n",
    "def delong_roc_variance(ground_truth, predictions, sample_weight=None):\n",
    "    \"\"\"\n",
    "    Computes ROC AUC variance for a single set of predictions\n",
    "    Args:\n",
    "       ground_truth: np.array of 0 and 1\n",
    "       predictions: np.array of floats of the probability of being class 1\n",
    "    \"\"\"\n",
    "    order, label_1_count, ordered_sample_weight = compute_ground_truth_statistics(\n",
    "        ground_truth, sample_weight)\n",
    "    predictions_sorted_transposed = predictions[np.newaxis, order]\n",
    "    aucs, delongcov = fastDeLong(predictions_sorted_transposed, label_1_count, ordered_sample_weight)\n",
    "    assert len(aucs) == 1, \"There is a bug in the code, please forward this to the developers\"\n",
    "    return aucs[0], delongcov\n",
    "\n",
    "\n",
    "def roc_auc_deLong_ci(y_true,y_pred):\n",
    "    alpha = .95\n",
    "    #y_pred = np.array([0.21, 0.32, 0.63, 0.35, 0.92, 0.79, 0.82, 0.99, 0.04])\n",
    "    #y_true = np.array([0,    1,    0,    0,    1,    1,    0,    1,    0   ])\n",
    "\n",
    "    auc, auc_cov = delong_roc_variance(\n",
    "        y_true,\n",
    "        y_pred)\n",
    "\n",
    "    auc_std = np.sqrt(auc_cov)\n",
    "    lower_upper_q = np.abs(np.array([0, 1]) - (1 - alpha) / 2)\n",
    "\n",
    "    ci = stats.norm.ppf(\n",
    "        lower_upper_q,\n",
    "        loc=auc,\n",
    "        scale=auc_std)\n",
    "\n",
    "    ci[ci > 1] = 1\n",
    "\n",
    "    print('AUC:', auc)\n",
    "    print('AUC COV:', auc_cov)\n",
    "    print('95% AUC CI:', ci)\n",
    "    return (str(round(auc, 2)) + \" (\" + str(round(ci[0], 2))+ \"-\"+ str(round(ci[1], 2))+ \")\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AUROC with CI\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from math import sqrt\n",
    "\n",
    "def roc_auc_ci(y_true, y_score):\n",
    "    positive=1\n",
    "    AUC = roc_auc_score(y_true, y_score)\n",
    "    N1 = sum(y_true == positive)\n",
    "    N2 = sum(y_true != positive)\n",
    "    Q1 = AUC / (2 - AUC)\n",
    "    Q2 = 2*AUC**2 / (1 + AUC)\n",
    "    SE_AUC = sqrt((AUC*(1 - AUC) + (N1 - 1)*(Q1 - AUC**2) + (N2 - 1)*(Q2 - AUC**2)) / (N1*N2))\n",
    "    lower = AUC - 1.96*SE_AUC\n",
    "    upper = AUC + 1.96*SE_AUC\n",
    "    if lower < 0:\n",
    "        lower = 0\n",
    "    if upper > 1:\n",
    "        upper = 1\n",
    "    return (str(AUC)+ \",\" +str(lower) +\"-\"+str(upper))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step.1 Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Clump_thickness</th>\n",
       "      <th>Uniformity_cell_size</th>\n",
       "      <th>Uniformity_cell_shape</th>\n",
       "      <th>Marginal_adhesion</th>\n",
       "      <th>Single_e_cell_size</th>\n",
       "      <th>Bare_nuclei</th>\n",
       "      <th>Bland_chromatin</th>\n",
       "      <th>Normal_nucleoli</th>\n",
       "      <th>Mitoses</th>\n",
       "      <th>LABEL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Clump_thickness  Uniformity_cell_size  Uniformity_cell_shape  \\\n",
       "0                5                     4                      4   \n",
       "1                3                     1                      1   \n",
       "2                6                     8                      8   \n",
       "3                4                     1                      1   \n",
       "4                8                    10                     10   \n",
       "\n",
       "   Marginal_adhesion  Single_e_cell_size Bare_nuclei  Bland_chromatin  \\\n",
       "0                  5                   7          10                3   \n",
       "1                  1                   2           2                3   \n",
       "2                  1                   3           4                3   \n",
       "3                  3                   2           1                3   \n",
       "4                  8                   7          10                9   \n",
       "\n",
       "   Normal_nucleoli  Mitoses LABEL  \n",
       "0                2        1     2  \n",
       "1                1        1     2  \n",
       "2                7        1     2  \n",
       "3                1        1     2  \n",
       "4                7        1     4  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# IMPORTANT: Target column should be named as 'LABEL'\n",
    "trainingData = pd.read_csv('trainingData.data')\n",
    "\n",
    "# IMPORTANT: The first column should be Patient ID \n",
    "trainingData.drop(columns=trainingData.columns[0], axis=1, inplace=True)\n",
    "\n",
    "#trainingData['LABEL'] = trainingData['LABEL'].str.replace('NO','0')\n",
    "#trainingData['LABEL'] = trainingData['LABEL'].str.replace('Y','1')\n",
    "trainingData['LABEL'] = trainingData['LABEL'].astype('category')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(trainingData.drop('LABEL', 1).values, trainingData['LABEL'].to_numpy(), test_size=0.3, random_state=16)\n",
    "\n",
    "trainingData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step.2 Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, roc_auc_score, roc_curve\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import multiprocessing\n",
    "from sklearn.metrics import classification_report, roc_auc_score, roc_curve\n",
    "\n",
    "# DEFINE PARAMETERS\n",
    "n_defined_jobs = multiprocessing.cpu_count() - 2\n",
    "param_grid = {\n",
    "    'n_estimators': [64, 256, 1024],\n",
    "    'max_depth': [8, 16, None],\n",
    "    'criterion': [\"gini\"]\n",
    "             }\n",
    "\n",
    "# TRAIN\n",
    "clf = RandomForestClassifier()\n",
    "classification_model = GridSearchCV(clf, param_grid, cv=5, n_jobs = n_defined_jobs)\n",
    "classification_model.fit(X_train, y_train)\n",
    "save_model(\"prediction_model\", classification_model, X_test, y_test, features)\n",
    " \n",
    "# TEST    \n",
    "y_pred = classification_model.predict_proba(X_test)\n",
    "y_pred_classes =  prediction_model.predict(X_test)\n",
    "model_auc = roc_auc_score(y_test, y_pred[:, 1])\n",
    "print(model_auc)\n",
    "classificationReport = classification_report(y_test, y_pred_classes)\n",
    "print(classificationReport)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step.3 Explain the Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_size = 100 # The amount of predictions to be explained\n",
    "data_to_be_explained = trainingData.drop('LABEL', 1).values\n",
    "isFullRun = False\n",
    "if isFullRun:\n",
    "    data_size = len(trainingData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. LIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lime import lime_tabular\n",
    "\n",
    "lime_explainer = lime_tabular.LimeTabularExplainer(data_to_be_explained\n",
    "                                               ,feature_names = features \n",
    "                                               ,class_names=['No','Yes']\n",
    "                                               ,mode='classification'\n",
    "                                               ,feature_selection= 'lasso_path' \n",
    "                                               ,discretize_continuous=True\n",
    "                                               ,discretizer='quartile'\n",
    "                                             )\n",
    "\n",
    "predict_fn = lambda x: prediction_model.predict_proba(x).astype(float)\n",
    "fi_df = pd.DataFrame(0.0, index=np.arange(data_size),columns=features)\n",
    "\n",
    "import datetime\n",
    "start = datetime.datetime.now()\n",
    "     \n",
    "for p in range(0,data_size):\n",
    "    lime_explanation = lime_explainer.explain_instance(data_to_be_explained[p]\n",
    "                                          ,predict_fn\n",
    "                                          ,num_features=10\n",
    "                                          #,top_labels=1\n",
    "                                          #,distance_metric='cosine'\n",
    "                                          #,distance_metric='manhattan'\n",
    "                                        )\n",
    "   \n",
    "    fi_lime = lime_explanation.as_list()\n",
    "    #print(fi_lime)\n",
    "    \n",
    "    fi_lime = pd.DataFrame(fi_lime, columns=['feature','importance'])\n",
    "    for i in range(0,len(fi_lime)): \n",
    "        splitted = fi_lime.loc[i, 'feature'].split(' ')\n",
    "        if len(splitted)==3:\n",
    "            fi_lime.loc[i, 'feature'] = splitted[0]\n",
    "        else:\n",
    "            fi_lime.loc[i, 'feature'] = splitted[2]\n",
    "    \n",
    "    for i in range(0,len(fi_lime)):\n",
    "        fi_df.loc[p,fi_lime['feature'][i]]= fi_lime.loc[i,'importance']\n",
    "\n",
    "end = datetime.datetime.now()\n",
    "print(end-start)\n",
    "\n",
    "save_data(\"LIME\",data_size, fi_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap \n",
    "shap.initjs()\n",
    "    \n",
    "# Define a tree explainer for the built model\n",
    "explainer_shap = shap.TreeExplainer(prediction_model.best_estimator_)\n",
    "\n",
    "fi_shap_df = pd.DataFrame(0.0, index=np.arange(data_size),columns=features)\n",
    "\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "for p in range(0,data_size):\n",
    "    # obtain shap values for the chosen row of the test data\n",
    "    shap_values = explainer_shap.shap_values(data_to_be_explained[p])\n",
    "   \n",
    "    #fi_shap = pd.DataFrame(shap_values, columns=['importance'])\n",
    "    for f in range(0,len(features)): \n",
    "        fi_shap_df.loc[p,features[f]]= shap_values[0][f]\n",
    "        \n",
    "end = datetime.datetime.now()\n",
    "print(end-start)\n",
    "\n",
    "save_data(\"SHAP\",data_size, fi_shap_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step.4 Evalute XAI Models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-6de6a41fba63>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# classification performance (EMC) table\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mauroc_results_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'0.0'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'PM'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'EM_LIME'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'EM_SHAP'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Performance (AUROC)'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# explanation performance (EMR) table\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mrmse_results_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'0.0'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'EM_LIME'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'EM_SHAP'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Error Rate (RMSE)'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "# Prepare Result Tables\n",
    "\n",
    "# 1- classification performance (EMC) table\n",
    "auroc_results_df = pd.DataFrame('0.0', index=['PM','EMC_LIME','EMC_SHAP'],columns=['Performance'])\n",
    "\n",
    "# 2- explanation performance (EMR) table\n",
    "rmse_results_df = pd.DataFrame('0.0', index=['EMR_LIME','EMR_SHAP'],columns=['ErrorRate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1- Evaluation Dataset (D)\n",
    "X_train_d, X_test_d, y_train_d, y_test_d = train_test_split(X_test[0:data_size], y_test[0:data_size], test_size=0.3, random_state=16)\n",
    "\n",
    "# 2- DFI with LIME \n",
    "fi_val_df = pd.DataFrame(0.0, index=np.arange(data_size),columns=features) # TODO: change size\n",
    "val_df = pd.DataFrame(X_test[0:data_size],columns=features)\n",
    "\n",
    "for f in features:\n",
    "    fi_val_df[f] = fi_df[f] * val_df[f]\n",
    "    \n",
    "    \n",
    "# 3- DFI with SHAP\n",
    "fi_shap_val_df = pd.DataFrame(0.0, index=np.arange(data_size),columns=features) # TODO: change size\n",
    "\n",
    "for f in features:\n",
    "    fi_shap_val_df[f] = fi_shap_df[f] * val_df[f]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 EMC (Evaluation Model - Classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.1 Classification performance without Explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'RandomForestClassifier' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-4dc42d3777d0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mclf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprediction_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mn_defined_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprediction_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_d\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train_d\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0my_pred_d\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprediction_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test_d\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0my_pred_classes_d\u001b[0m\u001b[1;33m=\u001b[0m  \u001b[0mprediction_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test_d\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'RandomForestClassifier' is not defined"
     ]
    }
   ],
   "source": [
    "clf = RandomForestClassifier()\n",
    "prediction_model = GridSearchCV(clf, param_grid, cv=5, n_jobs = n_defined_jobs) \n",
    "prediction_model.fit(X_train_d, y_train_d)\n",
    "y_pred_d = prediction_model.predict_proba(X_test_d)\n",
    "y_pred_classes_d=  prediction_model.predict(X_test_d)\n",
    "\n",
    "model_auc_pm = roc_auc_score(y_test_d, y_pred_d[:, 1])\n",
    "classificationReport_pm = classification_report(y_test_d, y_pred_classes_d)\n",
    "\n",
    "auroc_results_df.loc['PM','Performance'] = roc_auc_deLong_ci(np.array(y_test_d).astype(int), np.array(y_pred_d[:, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.2 Classification performance with LIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_eval2c, X_test_eval2c, y_train_eval2c, y_test_eval2c = train_test_split(fi_val_df.values, y_test[0:data_size], test_size=0.3, random_state=16)\n",
    "\n",
    "clf = RandomForestClassifier()\n",
    "eval_model2c = GridSearchCV(clf, param_grid, cv=5, n_jobs = n_defined_jobs) \n",
    "eval_model2c.fit(X_train_eval2c, y_train_eval2c)\n",
    "y_pred2c = eval_model2c.predict_proba(X_test_eval2c)\n",
    "y_pred_classes2c =  eval_model2c.predict(X_test_eval2c)\n",
    "\n",
    "model_auc2c = roc_auc_score(y_test_eval2c, y_pred2c[:, 1])\n",
    "classificationReport2c = classification_report(y_test_eval2c, y_pred_classes2c)\n",
    "auroc_results_df.loc['EM_LIME','FI*Values'] = roc_auc_deLong_ci(np.array(y_test_eval2c).astype(int), np.array(y_pred2c[:, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.3 Classification performance with SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_eval2sc, X_test_eval2sc, y_train_eval2sc, y_test_eval2sc = train_test_split(fi_shap_val_df.values, y_test[0:data_size], test_size=0.3, random_state=16)\n",
    "\n",
    "clf = RandomForestClassifier()\n",
    "eval_model2sc = GridSearchCV(clf, param_grid, cv=5, n_jobs = n_defined_jobs) \n",
    "eval_model2sc.fit(X_train_eval2sc, y_train_eval2sc)\n",
    "y_pred2sc = eval_model2sc.predict_proba(X_test_eval2sc)\n",
    "y_pred_classes2sc =  eval_model2sc.predict(X_test_eval2sc)\n",
    "\n",
    "model_auc2sc = roc_auc_score(y_test_eval2sc, y_pred2sc[:, 1])\n",
    "classificationReport2sc = classification_report(y_test_eval2sc, y_pred_classes2sc)\n",
    "auroc_results_df.loc['EM_SHAP','FI*Values'] = roc_auc_deLong_ci(np.array(y_test_eval2sc).astype(int), np.array(y_pred2sc[:, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 EMR (Evaluation Model - Regressor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.1 Explanation performance of LIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_eval2, X_test_eval2, y_train_eval2, y_test_eval2 = train_test_split(fi_val_df.values, y_pred[0:data_size], test_size=0.3, random_state=16)\n",
    "\n",
    "eval_model2 = RandomForestRegressor(max_depth=2, random_state=0)\n",
    "eval_model2.fit(X_train_eval2, y_train_eval2)\n",
    "save_model(\"eval_model2\", eval_model2, X_test_eval2, y_test_eval2, features)\n",
    "\n",
    "y_pred2 = eval_model2.predict(X_test_eval2)\n",
    "RMSE2 = math.sqrt(MSE2)\n",
    "rmse_results_df.loc['EM_LIME','FI*Values'] = RMSE2 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.2 Explanation performance of SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_eval2s, X_test_eval2s, y_train_eval2s, y_test_eval2s = train_test_split(fi_shap_val_df.values, y_pred[0:data_size], test_size=0.3, random_state=16)\n",
    "\n",
    "eval_model2s = RandomForestRegressor(max_depth=2, random_state=0)\n",
    "eval_model2s.fit(X_train_eval2s, y_train_eval2s)\n",
    "save_model(\"eval_model2s\", eval_model2s, X_test_eval2s, y_test_eval2s, features)\n",
    "\n",
    "y_pred2s = eval_model2s.predict(X_test_eval2s)\n",
    "RMSE2s = math.sqrt(MSE2s)\n",
    "rmse_results_df.loc['EM_SHAP','FI*Values'] = RMSE2s \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auroc_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_results_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "fd418446a9326e33059c5a832f971392064b382f0c1345c038d317674ac7eb90"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
